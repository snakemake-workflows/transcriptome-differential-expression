import os
from os import path
from collections import OrderedDict
import sys

from snakemake.utils import min_version

min_version("8.10.7")


localrules:
    merge_counts,
    write_coldata,
    write_de_params,
    de_analysis,
    dump_versions,
    info,


configfile: "config/config.yml"


include: "rules/commons.smk"
include: "rules/utils.smk"


inputdir = config["inputdir"]


rule all:
    input:
        ver=rules.dump_versions.output.ver,
        count_tsvs=expand("counts/{sample}_salmon/quant.sf", sample=samples["sample"]),
        merged_tsv="merged/all_counts.tsv",
        coldata="de_analysis/coldata.tsv",
        de_params="de_analysis/de_params.tsv",
        dispersion_graph="de_analysis/dispersion_graph.svg",
        ma_graph="de_analysis/ma_graph.svg",
        de_heatmap="de_analysis/heatmap.svg",
        lfc_analysis="de_analysis/lfc_analysis.csv",
        sample_QC=expand("QC/NanoPlot/{sample}.tar.gz", sample=samples["sample"]),
        samstats=expand("QC/samstats/{sample}.txt", sample=samples["sample"])


rule build_minimap_index:  ## build minimap2 index
    input:
        genome=config["transcriptome"],
    output:
        index="index/transcriptome_index.mmi",
    params:
        opts=config["minimap_index_opts"],
    log:
        "logs/minimap2/index.log",
    conda:
        "envs/env.yml"
    shell:
        """
    minimap2 -t {resources.cpus_per_task} {params.opts} -d {output.index} {input.genome} 2> {log}
    """


# mapping reads with minimap2
rule map_reads:
    input:
        index=rules.build_minimap_index.output.index,
        fastq=lambda wildcards: get_mapped_reads_input(
            samples["sample"][wildcards.sample]
        ),
    output:
        "alignments/{sample}.bam",
    log:
        "logs/minimap2/mapping_{sample}.log",
    params:
        opts=config["minimap2_opts"],
        msec=config["maximum_secondary"],
        psec=config["secondary_score_ratio"],
    conda:
        "envs/env.yml"
    shell:
        """
    minimap2 -t {resources.cpus_per_task} -ax map-ont -p {params.psec} -N {params.msec} {params.opts} {input.index} {input.fastq} > {output} 2> {log}
    """


#QC and metadata with NanoPlot
if config["summary"] == "None":
    rule plot_samples:
        input:
            fastq=lambda wildcards: get_mapped_reads_input(
                samples["sample"][wildcards.sample]
            ),
        output:
            directory("NanoPlot/{sample}")
        log:
            "logs/NanoPlot/{sample}.log"
        resources:
            ## max of 39 for our SLURM partition
            cpus_per_task= min(8,39) #problem with max(len(input.fastq),39)
        conda:
            "envs/env.yml"
        shell:
            "mkdir {output}; "
            "NanoPlot -t {resources.cpus_per_task} --tsv_stats -f svg "
            "--fastq {input.fastq} -o {output} 2> {log}"
else:
    rule plot_samples:
        input:
            summary = config["summary"]
        output:
            directory("NanoPlot")
        log:
            "logs/NanoPlot/NanoPlot.log"
        conda:
            "envs/env.yml"
        shell:
            "mkdir {output}/{wildcards.sample}; "
            "NanoPlot -t {resources.cpus_per_task} --barcoded --tsv_stats "
            "--summary {input.summary} -o {output} 2> {log}"


rule compress_nplot:
    input:
        samples=rules.plot_samples.output
    output:
        "QC/NanoPlot/{sample}.tar.gz"
    log:
        "logs/NanoPlot/compress_{sample}.log"
    shell:
        "tar zcvf {output} {input} 2> {log}"


rule sam_sort:
    input:
        sam=rules.map_reads.output,
    output:
        "sorted_alignments/{sample}.bam",
    log:
        "logs/samtools/samsort_{sample}.log",
    conda:
        "envs/env.yml"
    shell:
        "samtools sort -@ {resources.cpus_per_task} {input.sam} -o {output} -O bam &> {log}"


rule sam_index:
    input:
        sbam=rules.sam_sort.output,
    output:
        ibam="sorted_alignments/{sample}_index.bam",
    log:
        "logs/samtools/samindex_{sample}.log",
    conda:
        "envs/env.yml"
    shell:
        """
           samtools index -@ {resources.cpus_per_task} {input.sbam} &> {log};
           mv {input.sbam} {output.ibam}
        """


rule sam_stats:
    input:
        bam=rules.map_reads.output,
    output:
        "QC/samstats/{sample}.txt",
    log:
        "logs/samtools/samstats_{sample}.log"
    conda:
        "envs/env.yml"
    shell:
        """
            samtools stats -@ {resources.cpus_per_task} {input.bam} > {output} 2> {log}
        """


rule count_reads:
    input:
        bam=rules.sam_index.output.ibam,
        trs=config["transcriptome"],
    output:
        tsv="counts/{sample}_salmon/quant.sf",
    params:
        tsv_dir="counts/{sample}_salmon",
        libtype=config["salmon_libtype"],
    log:
        "logs/salmon/{sample}.log",
    conda:
        "envs/env.yml"
    shell:
        """
        salmon --no-version-check quant  -p {resources.cpus_per_task} \
        -t {input.trs} -l {params.libtype} -a {input.bam} -o {params.tsv_dir} &> {log}
    """


rule merge_counts:
    input:
        count_tsvs=expand("counts/{sample}_salmon/quant.sf", sample=samples["sample"]),
    output:
        "merged/all_counts.tsv",
    log:
        "logs/merge_count.log",
    script:
        "scripts/merge_count_tsvs.py"


rule write_coldata:
    output:
        coldata="de_analysis/coldata.tsv",
    run:
        with open(f"{output}", "w") as outfile:
            outstring = "\t".join(samples.head())
            outfile.write(outstring)


rule write_de_params:
    output:
        de_params="de_analysis/de_params.tsv",
    run:
        d = OrderedDict()
        d["Annotation"] = [config["annotation"]]
        d["min_samps_gene_expr"] = [config["min_samps_gene_expr"]]
        d["min_samps_feature_expr"] = [config["min_samps_feature_expr"]]
        d["min_gene_expr"] = [config["min_gene_expr"]]
        d["min_feature_expr"] = [config["min_feature_expr"]]
        df = pd.DataFrame(d)
        df.to_csv(output.de_params, sep="\t", index=False)


rule de_analysis:
    input:
        de_params=rules.write_de_params.output.de_params,
        coldata=rules.write_coldata.output.coldata,
        tsv=rules.merge_counts.output,
    output:
        dispersion_graph="de_analysis/dispersion_graph.svg",
        ma_graph="de_analysis/ma_graph.svg",
        de_heatmap="de_analysis/heatmap.svg",
        de_top_heatmap="de_analysis/heatmap_top.svg",
        lfc_analysis="de_analysis/lfc_analysis.csv",
    log:
        "logs/de_analysis.log",
    threads: 4
    conda:
        "envs/env.yml"
    script:
        "scripts/de_analysis.py"
